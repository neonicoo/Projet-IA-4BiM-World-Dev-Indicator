{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet IA - 4BiM - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auteurs : Aurélie Fischer et Nicolas Mendiboure \n",
    "#### Enseignants : Sergio Peignier et Chistophe Rigotti \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import math\n",
    "import sklearn\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "#import sergio_peignier as sp\n",
    "#import chistophe_rigotti as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lecture des données:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au départ, nous avons construit un jeu de données à partir du site http://databank.worldbank.org . Celui-ci contient diverses informations sur les pays du monde entier. Au total, nous avons alors 217 pays pour lesquels nous avons sélectionné 50 variables différentes, qui vont nous permettre de les caractériser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df = pds.read_csv(\"./datas/Data-IA-World-Development-Indicator.txt\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_df.head()\n",
    "#sparse_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques statistiques :\n",
    "sparse_df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Création des classes (continents) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour anticiper les analyses que nous allons effectuer sur le jeu de données, nous avions besoin d'établir des classes auxquelles chaque pays que l'on considère est susceptible d'appartenir. Il nous a paru naturel de choisir les continents pour classer chaque pays. Pour cela, on utilise une librairie *pycountry_convert* qui associe un code pays (par exemple \"CHN\" pour la Chine) à un code continent : \n",
    "\n",
    "* AS pour l'Asie\n",
    "* AF pour l'Afrique\n",
    "* NA pour l'Amérique du nord\n",
    "* OC pour l'Océanie\n",
    "* AN pour l'Antarctique\n",
    "* EU pour l'Europe\n",
    "* SA pour l'Amérique du sud\n",
    "\n",
    "On retient donc qu'on pourrait classer les pays dans 7 classes possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = sparse_df[[\"Country Name\", \"Country Code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_np = country_code[\"Country Code\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_continent(country_code):\n",
    "    res = []\n",
    "    for code3 in country_code :\n",
    "        if (code3 == \"CHI\"):\n",
    "            code2 = \"GB\"\n",
    "        elif (code3 == \"XKX\"):\n",
    "            code2 = \"XK\"\n",
    "        else:\n",
    "            code2 = pc.country_alpha3_to_country_alpha2(code3)\n",
    "            \n",
    "        \n",
    "        if (code2 == \"TL\"):\n",
    "            country_continent_code = \"AS\"\n",
    "        elif (code2 == \"SX\"):\n",
    "            country_continent_code = \"SA\"\n",
    "        else:\n",
    "            country_continent_code = pc.country_alpha2_to_continent_code(code2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "        res.append(country_continent_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = country_to_continent(country_code_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df[\"Continents\"] = continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Selection et filtrage des données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de pouvoir effectuer un clustering, on réalise une sélection des attributs ainsi que des pays (les objets étudiés ici) de sorte à ce qu'on ait assez d'informations à traiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Selection des attributs (colonnes) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par sélectionner les attributs. Ceux-ci sont choisis de sorte à ce qu'ils aient de l'information pour un maximum de pays possible. Dans l'idéal, on aimerait avoir de l'information pour chaque pays et chaque attribut, mais cela est très rare. Ainsi, nous avons défini une limite de nombre manquant de données (nombre de NaN) par attribut. Si l'attribut a un nombre de données manquantes supérieur à cette limite, alors on le considère plus dans notre étude. Cette limite a été fixée à 40 après plusieurs essais. Elle est suffisamment basse pour garder les colonnes contenant un maximum de données, tout en étant assez haute pour conserver une large gamme d'attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui supprime une colonne (attribut) si le nb de NaN >= limit\n",
    "\n",
    "def delete_NaN_col(df, limit):\n",
    "    NaN_col = df.isna().sum()\n",
    "    tmp = df.copy(deep=True) \n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        if NaN_col[i] >= limit:\n",
    "            df = df.drop(columns = tmp.columns[i])\n",
    "            \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df2 = delete_NaN_col(sparse_df, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlèvre les colonnes 'Time' et 'Time Code' qui ne nous interessent pas ici\n",
    "def remove_useless_col(df):\n",
    "    df = df.drop(columns = [\"Time\", \"Time Code\", \"Country Code\"])\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df2 = remove_useless_col(sparse_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation du nombre de donnees manquantes par colonne apres selection\n",
    "sparse_df2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Selection des objets (lignes) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suivant la même logique que pour les attributs, on sélectionne les objets, donc les pays, pour lesquels on a assez d'informations pour les attributs restants. \n",
    "\n",
    "Ainsi, on obtient un jeu de données assez complet où on pourra réellement comparer chaque objet en fonction de comment il est caractérisé par les différents attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui supprime une ligne (object) si le nb de NaN >= limit\n",
    "\n",
    "def delete_NaN_row(df, limit):\n",
    "    NaN_row = df.isna().sum(axis=1)\n",
    "    tmp = df.copy(deep=True) #temporary df\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        if NaN_row[i] >= limit:\n",
    "            df = df.drop([i], axis = 0)\n",
    "            \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df3 = delete_NaN_row(sparse_df2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On remplace les indexes des ligne {0, 1, 2, ..n} par les noms de pays\n",
    "#Puis on supprime la colonne \"Country Name\" pour n'avoir plus que des valeurs numeriques\n",
    "sparse_df3.index = sparse_df3['Country Name']\n",
    "sparse_df3 = sparse_df3.drop(columns = ['Country Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation du jeu de donnees apres selection des attributs et des objets\n",
    "sparse_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Études des correlations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après sélection des objets et des attributs selon la quantité d'informations manquantes, on va étudier la corrélation présente au sein des données. Cela permet de se rendre compte de la redondance de l'information qui y est présente. L'objectif est alors d'éviter cette redondance en ne considérant uniquement des attributs faiblement corrélés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mesure de la correlation de nos donnees\n",
    "corr_df = sparse_df3.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation graphique des correlations\n",
    "plt.figure(figsize= (15, 12))\n",
    "sns.heatmap(corr_df,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(corr_df,\n",
    "               figsize= (16, 12),\n",
    "               annot=True,\n",
    "               dendrogram_ratio=(0.1, 0.2),\n",
    "               row_cluster=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_corr(df, limit):\n",
    "    #On construit notre matrice de correlation en valeur absolue\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    #Triangle supérieur de la matrice de corrélation : \n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > limit)]\n",
    "    df = df.drop(df[to_drop], axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection des attributs avec une limite de correlation = 55%\n",
    "df = delete_corr(sparse_df3, 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation graphique des correlations après selection\n",
    "plt.figure(figsize= (15, 12))\n",
    "sns.heatmap(df.corr().abs(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme il nous reste une dizaine d'attributs, nous avons décidé de retirer à la main la colonne \"Crédit net domestique\" dont on ne savait pas réellement à quoi elle correspondait et qu'il aurait fallu normaliser par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Net domestic credit (current LCU) [FM.AST.DOMS.CN]\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On renomme les labels des colonnes pour plus de fluidite\n",
    "df = df.rename(columns={\"Access to clean fuels and technologies for cooking (% of population) [EG.CFT.ACCS.ZS]\" : \"Clean fuels and technologies for cooking access (% pop)\"})\n",
    "df = df.rename(columns={\"Aquaculture production (metric tons) [ER.FSH.AQUA.MT]\" : \"Aquaculture prod (metric tons)\"})\n",
    "df = df.rename(columns={\"Compulsory education, duration (years) [SE.COM.DURS]\" : \"Compulsory education duration (years)\"})\n",
    "df = df.rename(columns={\"Death rate, crude (per 1,000 people) [SP.DYN.CDRT.IN]\" : \"Death rate (per 10e3 )\"})\n",
    "df = df.rename(columns={\"Incidence of tuberculosis (per 10e5 people) [SH.TBS.INCD]\" : \"Incidence of tuberculosis (per 100 000)\"})\n",
    "df = df.rename(columns={\"Number of infant deaths [SH.DTH.IMRT]\" : \"Number of infant deaths\"})\n",
    "df = df.rename(columns={\"Preprimary education, duration (years) [SE.PRE.DURS]\" : \"Preprimary education duration (years)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation du jeu de donnees après toutes les sélections d'attributs et d'objets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cette étape-ci, nous avons pu sélectionner tous les attributs et tous les objets de notre étude. Le jeu de données que nous allons analyser est donc composer de **133 pays et de 7 attributs différents** (la huitième colonne correspond aux continents, donc aux classes que nous avons rajouté nous-mêmes au début de l'étude): \n",
    "\n",
    "* Access to clean fuels and technologies for cooking (% of population) [EG.CFT.ACCS.ZS]\n",
    "* Aquaculture production (metric tons) [ER.FSH.AQUA.MT]\n",
    "* Compulsory education, duration (years) [SE.COM.DURS]\n",
    "* Death rate, crude (per 1,000 people) [SP.DYN.CDRT.IN]\n",
    "* Incidence of tuberculosis (per 10e5 people) [SH.TBS.INCD]\n",
    "* Number of infant deaths [SH.DTH.IMRT]\n",
    "* Preprimary education, duration (years) [SE.PRE.DURS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Normalisation relative (aux surfaces et populations par pays) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données obtenus contient deux attributs que l'on doit normaliser, puisqu'ils ne sont pas relatifs à la distribution du pays, alors que les autres le sont. Il s'agit de la **production d'aquaculture** que l'on va chercher à normaliser par rapport la surface du pays, et du **nombre de morts infantiles**, que l'on va normaliser par rapport à la taille de la population du pays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_country(df, df_ref):\n",
    "    tmp = df.copy(deep = True)\n",
    "    for i, country in  enumerate( tmp[\"Country Name\"]):\n",
    "        if country not in df_ref.index.values:\n",
    "            df = df.drop(tmp.index[i], axis = 0)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère les données de la taille de la population uniquement pour les 133 pays étudiés\n",
    "population_size = pds.read_csv(\"./datas/Popula-schtroumpf/Population-size-per-country.txt\", sep=\"\\t\", header=0)\n",
    "population_size = remove_useless_col(population_size)\n",
    "population_size = delete_NaN_row(population_size, 1)\n",
    "population_size = remove_country(population_size, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère les données de la surface du territoire uniquement pour les 133 pays étudiés\n",
    "surfaces = pds.read_csv(\"./datas/Surfa-schtroumpf/Country-surfaces.txt\", sep=\"\\t\", header=0)\n",
    "surfaces = remove_useless_col(surfaces)\n",
    "surfaces = delete_NaN_row(surfaces, 1)\n",
    "surfaces = remove_country(surfaces, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(population_size) == len(df), len(surfaces)== len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy(deep = True)\n",
    "print(\"Avant la normalisation relative des données :\", \"\\n\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[:, 1] =  df2.iloc[:, 1].values/surfaces.iloc[:, 1].values\n",
    "df2 = df2.rename(columns={\"Aquaculture prod (metric tons)\" : \"Aquaculture prod (metric tons/m²)\"})\n",
    "df2.iloc[:, 5] =  df2.iloc[:, 5].values/population_size.iloc[:, 1].values\n",
    "df2 = df2.rename(columns={\"Number of infant deaths\" : \"Rate of infant deaths in the population\"})\n",
    "\n",
    "print(\"Après la normalisation relative des données :\", \"\\n\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Normalisation par centrage - réduction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour terminer la partie de traitement de données, nous décidons de normaliser les données par centrage-réduction, selon la formule suivante : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    X = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En effet en visualisant les données sans cette normalisation, on voit que les ordres de grandeur, ainsi que les variances pour chaque attribut ne sont pas homogènes. On va donc centrer et réduire nos données pour corriger ce défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation des donnees sans centrage-reduction\n",
    "sns.pairplot(data=df2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de boxplot des donnees sans centrage-reduction\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.boxplot(data = df2)\n",
    "plt.show()\n",
    "\n",
    "#On voit que les ordres de grandeur de nos données ainsi que leur variances ne sont pas du tout homogènes.\n",
    "#On va donc les normaliser (centrer - réduire) pour corriger ce défaut  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui centre et réduit une colonne de données pour les normaliser\n",
    "def normalizer (data):\n",
    "    # Quelques statistiques :\n",
    "    mean = np.mean(data)\n",
    "    var = np.var(data)\n",
    "    sd = math.sqrt(var)\n",
    "    \n",
    "    norm = []\n",
    "    for x in data:\n",
    "        norm.append((x-mean)/sd)\n",
    "    \n",
    "    return (norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_whole_df (df):\n",
    "    normed_df = df.copy(deep = True)\n",
    "    for i in df.columns:\n",
    "        normed_df[i] = normalizer(df[i])\n",
    "        \n",
    "    return (normed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_df = norm_whole_df(df2.drop(columns = \"Continents\" ))\n",
    "normed_df[\"Continents\"] = df2[\"Continents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de boxplot des donnees après centrage-reduction\n",
    "plt.figure(figsize=(12,15))\n",
    "sns.boxplot(data = normed_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le boxplot ainsi obtenu nous permet d'avoir une vue sur les possibles points \"outliers\" qui pourraient nous gêner pour l'analyse. On cherche donc à enlever les pays qui y sont associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_outliers_row(df, limit):\n",
    "    z_scores = stats.zscore(df.drop(columns = \"Continents\" )) #calcule le z-score de df\n",
    "    abs_z_scores = np.abs(z_scores) # on passe en valeurs absolues\n",
    "    filtered_outliers = (abs_z_scores < limit).all(axis=1) # on ne garde que les lignes avec des valeurs > limit\n",
    "    new_df = df[filtered_outliers] #nouveau df\n",
    "    return (new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = search_outliers_row(normed_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,15))\n",
    "sns.boxplot(data = df3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplot après standardisation\n",
    "sns.pairplot(data=df3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On refait ci-dessous des statistiques basiques sur notre nouveau jeu de données df3. On voit bien que notre moyenne est centrée sur 0 et que notre écart-type varie autour de 1 pour chaque attribut. La normalisation a donc fonctionné. De plus, on voit bien sur le boxplot qu'on a enlevé les outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df3.describe()\n",
    "df_stats = df_stats.drop('count',axis=0)\n",
    "sns.heatmap(df_stats,annot=True,cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, le jeu de données que l'on va étudier contient **123 pays et 7 attributs**, présentés en section 3.3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 K-means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons notre analyse par la méthode K-Means. On cherche à identifier des clusters parmi nos données, sans avoir de classes référentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Création des clusters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.drop(columns = \"Continents\" )\n",
    "classes = df3.Continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utilisé le jeu de données fraîchement filtré et normalisé df4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un objet KMeans en cherchant 4 clusters:\n",
    "km = KMeans(n_clusters=4, init='k-means++',n_init=10, random_state=50, max_iter=300)\n",
    "km.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On stocke les clusters\n",
    "km_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On stocke les centroides obtenus :\n",
    "centroids = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les prédictions de K-Means sur le jeu de données\n",
    "predict = km.predict(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de K-Means a sorti des prédictions de clusters pour notre jeu de données. Il a rangé chaque pays dans un des quatre clusters possibles, comme on le voit ci-dessous. Notons qu'il est possible de lancer K-Means en réglant le nombre de clusters. Un de nos objectifs sera d'établir le nombre de clusters optimal pour nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut visualiser les clusters obtenus dans l'espace pour les situer les uns par rapport aux autres, en se plaçant sur un plan 2D. Ici, on s'est placé par exemple dans le plan présentant la \"Compulsory education duration\" en fonction du \"Clean fuels and technologies for cooking access\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des clusters \n",
    "plt.scatter(df4[\"Clean fuels and technologies for cooking access (% pop)\"], df4[\"Compulsory education duration (years)\"], c=km_clusters)\n",
    "plt.title('Data in Space', fontsize=14)\n",
    "plt.xlabel(\"Clean fuels and technologies for cooking access (% pop)\",fontsize=14)\n",
    "plt.ylabel(\"Compulsory education duration (years)\",fontsize=14 )\n",
    "plt.scatter(centroids[:, 0], centroids[:, 2], c='red',s=150, alpha=0.4)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Mesures sur nos kmeans :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons exécuté le k-means sur nos données, nous allons analyser ce que l'algorithme a produit comme résultats à travers diverses mesures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "Counter(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant par exemple simplement le nombre de pays qui ont été rangés par clusters, on peut se dire qu'ils sont répartis plus ou moins équitablement dans les différents clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) SSE :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La SSE est un critère qui nous permet d'évaluer les clusters de points formés par K-Means. Elle évalue la distance de chaque objet à son centroïde le plus proche. Ainsi, plus la SSE est grande, moins le K-Means est de bonne qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE=km.inertia_\n",
    "print(SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on a une SSE qui vaut 284, ce qui est plutôt grand. On peut essayer de réduire ce score en augmentant le nombre de clusters. Ainsi, chaque objet aura plus de chances d'être associé à un centroïde plus proche de lui. Affichons la courbe de la SSE en fonction du nombre de clusters avec lequel on règle l'algorithme K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Courbe de la SSE en fonction du nombre de clusters\n",
    "SSE_liste=[]\n",
    "for i in range(2,10):\n",
    "    km2=KMeans(n_clusters=i, init='k-means++',  n_init=1, random_state=50, max_iter=1000).fit(df4)\n",
    "    SSE_liste.append(km2.inertia_)\n",
    "    \n",
    "#print(SSE_liste)\n",
    "\n",
    "SSE_liste_random=[]\n",
    "x=[]\n",
    "for i in range(2,10):\n",
    "    km3=KMeans(n_clusters=i, init='random',  n_init=1, random_state=50, max_iter=1000).fit(df4)\n",
    "    SSE_liste_random.append(km3.inertia_)\n",
    "    x.append(i)\n",
    "    \n",
    "#print(SSE_liste_random)\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.plot(x, SSE_liste,label=\"k-means++\")\n",
    "plt.plot(x, SSE_liste_random,label=\"Random\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur cette courbe, on peut voir que la SSE diminue avec le nombre de clusters. Le choix du nombre de cluster optimal pour faire tourner K-Means fait qu'on cherche à réduire la SSE et à prendre un nombre de cluster assez grand. Toutefois, celui-ci ne doit pas être trop important non plus, au risque d'avoir trop de clusters et un surregroupement de nos données. La courbe inclut un coude pour un nombre de clusters valant 7. C'est donc cette valeur que l'on peut estimer comme optimale selon le critère de SSE pour faire tourner K-Means sur nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous avions défini des classes auxquelles pourraient appartenir les pays étudiés, nous allons comparer les 4 clusters obtenus avec K-Means avec les classes définies (ces classes sont les continents du monde pour rappel). Voici donc la table de contingence avec les différents clusters et les classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_crosstab = pds.crosstab(km_clusters,classes)\n",
    "sns.heatmap(km_crosstab, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate différentes choses sur cette table de contingence : \n",
    "\n",
    "* le cluster 0 est apparié globalement à l'Asie, à l'Amérique du Nord et l'Amérique du Sud\n",
    "* le cluster 1 est associé plutôt à l'Afrique\n",
    "* le cluster 2 est plutôt représentatif de l'Asie et de l'Europe\n",
    "* le cluster 3 représente globalement le continent européen\n",
    "\n",
    "Aucun pays ne se trouve en Antarctique, donc on a que 6 classes (continents) possibles pour les pays étudiés. C'est intéressant de situer les regroupements faits par K-Means. Certains continents comme l'Europe ou l'Afrique sont très bien dissociés des autres. D'autres, comme l'Amérique du nord et du sud sont appariés, ce qui paraît géographiquement parlant plausible, même si les situations économiques et politiques des pays se trouvant sur ces deux continents sont tout de même très différentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Mesures d'entropie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropie des clusters (c) comparés aux différentes classes (j):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\sum{p(j|c) \\times log(p(j|c)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(j|c)$ est la fréquence d'apparition de la classe j dans le cluster c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = km_crosstab.values/km_crosstab.values.sum(axis=1, keepdims=True) # divide each element of a row by the sum of the row\n",
    "entropy = [stats.entropy(row, base=2) for row in proba]\n",
    "print(\"entropy of each cluster: \", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Mesure de pureté :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer la pureté, chaque cluster est attribué à la classe qui est la plus fréquente dans ce cluster, puis la précision de cette attribution est mesurée en comptant le nombre d'éléments correctement attribués. Il s'agit donc de mesurer la précision d'un cluster à ne contenir les objets que d'une seule classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En réalité il s'agit d'une terminologie dérivant de l'entropie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$purete = \\sum_{j=1}^{K} \\frac{m_c}{m} \\times purete(c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $purete(c) = max(p(j,c))$;\n",
    "- $p(j,c)$ est la probabilité qu'un membre du cluster $c$ appartienne à la classe $j$;\n",
    "- $m_c$ est la taille du cluster $c$;\n",
    "- $m$ est la taille totale du nombre d'éléments (points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(classes, predictions):\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(classes, predictions)\n",
    "    return ( np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_score(classes, km_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Remarque :</ins> Une grande pureté est facile à atteindre lorsque le nombre de clusters formés devient important. En particulier, la pureté tend vers $1$ si le nombre de cluster tend à être égale aux nombre de points. Ainsi, nous ne pouvons pas utiliser la pureté pour faire un compromis entre la qualité du regroupement et le nombre de regroupements.\n",
    "\n",
    "Une mesure qui nous permet de faire ce compromis est l'information mutuelle normalisée ou INM : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Information mutuelle normalisée (IMN) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IMN(j,c) = \\frac{2 \\times I(j, c)}{E(j) + E(c)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $j$ = label de la classe ;\n",
    "- $c$ = label du cluster ;\n",
    "- $E()$ = entropie ;\n",
    "- $I(j, c)$ = information mutuelle entre j et c. $I(j, c) = E(j) - E(j|c)$, où $E(j|c)$ désigne une entropie conditionnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.normalized_mutual_info_score(classes, km_clusters, average_method='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Coefficient de silhouette :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coefficient de silhouette pour un  point correspond à la différence entre sa distance moyenne avec les points du même cluster que lui (**cohésion**) et la distance moyenne avec les points des autres clusters voisins (**séparation**). Il est compris en $-1$ et $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Si on a une différence négative, le point est en moyenne plus proche du groupe voisin que du sien ;\n",
    " - Si on a une différence positive, le point est en moyenne plus proche de son groupe que du groupe voisin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Silh = metrics.silhouette_score(df4.values, \n",
    "                         km_clusters, \n",
    "                         metric='euclidean', \n",
    "                         sample_size=None) \n",
    "print(\"Coefficient de silhouette pour nos 3 clusters : \",Silh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le code suivant nous pouvons observer les silhouettes de chaque cluster, pour un nombre total de  2, 3, 4, 5, 6 et 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15,10))\n",
    "for i in [2, 3, 4, 5, 6, 7]:\n",
    "    \n",
    "    km_simu = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "    q, mod = divmod(i, 2) # permet d'attribuer les axes pour les subplots \n",
    "    \n",
    "    visualizer = SilhouetteVisualizer(km_simu, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Stabilité de la convergence de nos k-means :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convergence d'un algorithme K-Means peut être vue comme étant la stabilisation des centroids des clusters (les centroids ne bougent plus lors des itérations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Calcul de la stabilité pour la convergence de notre K-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_silhouette_coef = []\n",
    "sse_list = []\n",
    "k = 6\n",
    "for i in range(600):\n",
    "    km = KMeans(n_clusters=k, init='random', n_init=1)\n",
    "    km.fit(df4)\n",
    "    labels = km.predict(df4)\n",
    "    sse_list.append(np.sqrt(km.inertia_))\n",
    "    avg_silhouette_coef.append(metrics.silhouette_score(df4, labels,metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(sse_list, edgecolor = \"black\")\n",
    "plt.title(\"SSE\", fontsize = 18)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(avg_silhouette_coef, edgecolor = \"black\")\n",
    "plt.title(\"Average silhouette coeff\", fontsize = 18)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.boxplot(sse_list)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.boxplot(avg_silhouette_coef)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Même chose mais avec un nombre d'executions interne plus grand :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous allons effectuer la même opération qu'au dessus en changeant la valeur du *n_init* par une valeur plus élevée. Pour rappel le paramètre *n_init* représente le nombre de fois où l'algorithme du k-means sera exécuté avec différentes graines pour les centroïdes. Le meilleur résltat en terme d'inertie sera retenue parmi tous les *n_init*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_silhouette_coef = []\n",
    "sse_list = []\n",
    "k = 6\n",
    "for i in range(600):\n",
    "    km = KMeans(n_clusters=k, init='random', n_init = 30)\n",
    "    km.fit(df4)\n",
    "    labels = km.predict(df4)\n",
    "    sse_list.append(np.sqrt(km.inertia_))\n",
    "    avg_silhouette_coef.append(metrics.silhouette_score(df4, labels,metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(sse_list, color='coral', edgecolor = \"black\")\n",
    "plt.title(\"SSE\", fontsize = 18)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(avg_silhouette_coef, color = 'coral', edgecolor = \"black\")\n",
    "plt.title(\"Average silhouette coeff\", fontsize = 18)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Evaluation de la stabilité de convergence un nombre de clusters différents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stability(km,df,iterations=100):\n",
    "    avg_silhouette_coef = []\n",
    "    sse_list = []\n",
    "    for i in range(500):\n",
    "        km.fit(df)\n",
    "        labels = km.predict(df)\n",
    "        avg_silhouette_coef.append(metrics.silhouette_score(df, labels,metric='euclidean'))\n",
    "    avg_silhouette_coef = np.asarray(avg_silhouette_coef)\n",
    "    return(avg_silhouette_coef.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability = []\n",
    "Ks = range(2,12)\n",
    "for k in Ks:\n",
    "    km = KMeans(n_clusters=k,init='random',n_init=5)\n",
    "    stability.append(compute_stability(km,df4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ks,stability,\"o-\")\n",
    "plt.xlabel(\"number of clusters \",fontsize=20)\n",
    "plt.ylabel(\"Avg. Silhouette Coef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le nombre de clusters pour une stabilité optimale est : \", int(stability.index(max(stability))) + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Hierachical clustering :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Méthode : \"complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_complete = sch.linkage(df4, method = 'complete', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "dendro = sch.dendrogram(z_complete, \n",
    "                        orientation='right', \n",
    "                        leaf_rotation=0, \n",
    "                        leaf_font_size=14,\n",
    "                        labels = df4.index)\n",
    "plt.title(\"Dendrogram with complete methode\")\n",
    "plt.ylabel(\"Countries\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Méthode : \"single\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_single = sch.linkage(df4, method = 'single', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "dendro = sch.dendrogram(z_single, \n",
    "                        orientation='right', \n",
    "                        leaf_rotation=0, \n",
    "                        leaf_font_size=14,\n",
    "                        labels = df4.index)\n",
    "plt.title(\"Dendrogram with single methode\")\n",
    "plt.ylabel(\"Countries\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd derivative of the distances\n",
    "\n",
    "acceleration_complete = np.diff(z_complete[:,2], 2)\n",
    "acceleration_single = np.diff(z_single[:,2], 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32, 18))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(z_single[:, 2], 'o-', color = \"blue\")\n",
    "plt.plot(z_complete[:, 2], 'o-', color = \"orange\")\n",
    "plt.legend([\"single\",\"complete\"])\n",
    "plt.ylabel(\"Hauteur\", fontsize = 22)\n",
    "plt.xlabel(\"Pays par ordre croissant\", fontsize = 22)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(z_complete[:, 2], 'o-', color = \"orange\")\n",
    "plt.plot(acceleration_complete, 'o-', color='green')\n",
    "plt.ylabel(\"Hauteur\", fontsize = 22)\n",
    "plt.legend([\"complete\",\"dérivée seconde de complete\"], fontsize = 22)\n",
    "plt.xlabel(\"Pays par ordre croissant\", fontsize = 22)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(z_single[:, 2], 'o-', color = \"blue\")\n",
    "plt.plot(acceleration_complete, 'o-', color='green')\n",
    "plt.legend([\"single\",\"dérivée seconde de signle\"], fontsize = 22)\n",
    "plt.ylabel(\"Hauteur\", fontsize = 22)\n",
    "plt.xlabel(\"Pays par ordre croissant\", fontsize = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 DB scan :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une méthode pour estimer le meilleur paramètre **eps** pour notre DBscan serait de calculer la distance de chaque point par rapport à son voisin le plus proche en utilisant les **NearestNeighbors**. Le point lui-même est inclus dans n_neighbors. Nous obtenons ensuite deux tableaux:\n",
    "\n",
    "- Un qui contient la distance aux points n_neighbors les plus proches ;\n",
    "- Le deuxième contient l'indice pour chacun de ces points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(df4.values)\n",
    "distances, indices = nbrs.kneighbors(df4.values)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ordonner nos résultats et les afficher dans un plot semblable à ceux déja vu pour des les SSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.subplot(111)\n",
    "eps_curve = ax.plot(distances, 'o-', color='b')\n",
    "plt.ylabel(\"eps\", fontsize = 18)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin la valeur optimale pour le paramètre **eps** trouve à la courbure maximale de la courbe, lu en ordonnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le déterminer nous allons importer la librairie Kneed qui va utiliser la méthode \"des genoux\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "points = eps_curve[0].get_data() # on récupère les points de la courbe (x et y)\n",
    "\n",
    "kneedle = KneeLocator(points[0], points[1], S=1.0, curve=\"convex\", direction=\"increasing\")\n",
    "knee = int( kneedle.knee )\n",
    "print(\"Meilleur paramètre epsilon  : \", round(points[1][knee], 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = 1.478, min_samples=4)\n",
    "dbscan.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_clusters = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table of species vs cluster labels\n",
    "crosstab = pds.crosstab(db_clusters,classes)\n",
    "sns.heatmap(crosstab, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster views in 2D projections\n",
    "\n",
    "db_results = df4.copy()\n",
    "db_results['cluster']= db_clusters\n",
    "classes_map = classes.map({'Africa':0, 'Asia':1, 'Europe':2, 'North America': 3, 'South America':4})\n",
    "db_results['class']= classes_map.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=db_results,hue='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Decision tree :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, classes_train, classes_test = sklearn.model_selection.train_test_split(df4, df3.Continents,\n",
    "                                                             test_size=1/3, \n",
    "                                                             train_size=2/3,\n",
    "                                                             random_state=None, \n",
    "                                                             shuffle=True, \n",
    "                                                             stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(df_train, classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_predicted = clf.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.crosstab(classes_test, classes_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre façon d'avoir la même table :\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(classes_test, classes_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score (classes_test, classes_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "tree.plot_tree(clf,filled=True,class_names=clf.classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre visualisation plus sympa utilisant graphviz  (décommenter pour visualier):\n",
    "\n",
    "tree_plot = tree.export_graphviz(clf, out_file = None, \n",
    "                           feature_names = df_train.columns,\n",
    "                           class_names = clf.classes_, \n",
    "                           filled=True, rounded = True,\n",
    "                           special_characters = True)\n",
    "\n",
    "graph = graphviz.Source(tree_plot)\n",
    "#display ( graph ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Validation croisée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold #cross-validation splitter\n",
    "from sklearn.model_selection import cross_validate #cross-validation evaluation of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['accuracy', \n",
    "         'precision_macro', \n",
    "         'precision_weighted', \n",
    "         'recall_macro', \n",
    "         'recall_weighted', \n",
    "         'f1_macro', \n",
    "         'f1_weighted']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "cv_scores = cross_validate(clf, df4, classes, scoring = tests, cv = cv, return_train_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.DataFrame(cv_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
